{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment binary classification through just one hidden layer definition using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of `binary` sentiment classification process:\n",
    "\n",
    " * Create three Counter objects to store positive, negative and total counts\n",
    " \n",
    " * Normalize for the effect of common words like the, is, ... that do not convey any sentiment load through calculating `np.log(pos_to_neg_ratio)` counter to focus on the words found in positive reviews more often than in negative reviews, and vice versa. As a result, neutral words will be close to `0`, words will get more positive as their ratios approach and go above `1`, and words will get more negative as their ratios approach and go below `-1`.\n",
    " \n",
    " * Tokenize review and label words by transforming them into numbers:\n",
    " >* Create a set of unique words from all reviews as the vocabulary set (use set) \n",
    " >* Map each unique words to an integer (use enumerate)\n",
    " >* Map labels to 0 or 1.\n",
    " \n",
    " * Reduce noise by strategically reducing the Vocabulary such that important sentiments stand out:\n",
    " >* Apply a `min_count' so that only words higher that cut-off are added to the vocabulary (avoid rare words and reduce noise). \n",
    " >* Apply a `polarity_cutoff` so that only words with their postive-to-negative higher than the cut-off are added to the vocabulary (discard neutral words as possible).  \n",
    " \n",
    " * Build the network efficiently:\n",
    " >* Find the indecis of all words in the input review from the processed training vocabulary. \n",
    " >* Update hidden layer just by updating the weights of elements that correspond to the indecis of the input words (discrad tonnes of unnecessary matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "eba2b193-0419-431e-8db9-60f34dd3fe83"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\") # [:80] limits the number of characters of each review per line\n",
    "\n",
    "g = open('data/reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1].lower(),g.readlines())) # .lower() methods makes all words lower case for consistent treatment.\n",
    "g.close()\n",
    "\n",
    "g = open('data/labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from collections import Counter # Counter is a convenient fast dictionary that already includes the original keys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Encapsulate our neural network in a class\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \"\"\"Create a SentimenNetwork with the given settings\n",
    "        Args:\n",
    "            reviews(list) - List of reviews used for training\n",
    "            labels(list) - List of POSITIVE/NEGATIVE labels associated with the given reviews\n",
    "            min_count(int) - Words should only be added to the vocabulary \n",
    "                             if they occur more than this many times\n",
    "            polarity_cutoff(float) - The absolute value of a word's positive-to-negative\n",
    "                                     ratio must be at least this big to be considered.\n",
    "            hidden_nodes(int) - Number of nodes to create in the hidden layer\n",
    "            learning_rate(float) - Learning rate to use while training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Assign a seed to our random number generator to ensure we get \n",
    "        # reproducable results during development \n",
    "        np.random.seed(1)\n",
    "\n",
    "        # process the reviews and their associated labels so that everything \n",
    "        # is ready for training\n",
    "        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)\n",
    "        \n",
    "        # Build the network to have the number of hidden nodes and the learning rate that\n",
    "        # were passed into this initializer. Make the same number of input nodes as\n",
    "        # there are vocabulary words and create a single output node.\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
    "        ## Calculate positive-to-negative ratios for words before building vocabulary\n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50): # the pos_neg_ratio ratio is not calculatd for a term that happens less than this count in a huge dataset \n",
    "                           # (regarless of being actually very positive or negative in few reviwes) .  \n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        \n",
    "        # populate review_vocab with all of the words in the given reviews.\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                # only add words that occur at least min_count times\n",
    "                # and for words with pos/neg ratios, only add words that meet the polarity_cutoff\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()): # recall that for a word to be in these pos_neg_ratios.keys(),  \n",
    "                                                       # it should have repeated more than a specific cnt! \n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "        \n",
    "        # Convert the vocabulary set to a list so we can access words via indices\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # populate label_vocab with all of the words in the given labels.\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        # Convert the label vocabulary set to a list so we can access labels via indices\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of the review and label vocabularies.\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Create a dictionary of words in the vocabulary mapped to index positions\n",
    "        self.word2index = {}\n",
    "        for index, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = index # populate self.word2index with indices for all the words in self.review_vocab\n",
    "        \n",
    "        # Create a dictionary of labels mapped to index positions\n",
    "        self.label2index = {}\n",
    "        for index, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = index  \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Store the number of nodes in input, hidden, and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Store the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        \n",
    "        # initialize self.weights_0_1 as a matrix of zeros. \n",
    "        # These are the weights between the input layer and the hidden layer.\n",
    "        self.weights_0_1 = np.zeros(shape=(self.input_nodes, self.hidden_nodes))\n",
    "        \n",
    "        # initialize self.weights_1_2 as a matrix of random values. \n",
    "        # These are the weights between the hidden layer and the output layer.\n",
    "        self.weights_1_2 = np.random.normal(loc=0.0, scale=self.hidden_nodes**-0.5, size=(self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        # The input layer, a two-dimensional matrix with shape 1 x hidden_nodes.\n",
    "        self.layer_1 = np.zeros((1, hidden_nodes))\n",
    "                \n",
    "    def get_target_for_label(self,label):\n",
    "        if (label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1+np.exp(-x)) \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        # Return the derivative of the sigmoid activation function, \n",
    "        # where \"output\" is the original output from the sigmoid fucntion \n",
    "        return output * (1- output)\n",
    "\n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        ## pre-process training reviews so we can deal directly with the indices of non-zero inputs\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                # if the particular word in the review exits in word2index vocabulary, \n",
    "                # find its index and add the word index to the indices set\n",
    "                if (word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word]) # add method works with set\n",
    "            # at the end of the loop, indices of all words are found from word2index vocabulary.\n",
    "            # indices set is converted to list and its elements are appended to training_reviews list.\n",
    "            # TAKE IMPORTANT NOTE that training_reviews is just a list of lists where each list is indices of words for a review.\n",
    "            training_reviews.append(list(indices))  \n",
    "        \n",
    "        # make sure out we have a matching number of reviews and labels\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        # Keep track of correct predictions to display accuracy during training \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        # Remember when we started for printing time statistics\n",
    "        start = time.time()\n",
    "\n",
    "        # loop through all the given reviews and run a forward and backward pass,\n",
    "        # updating weights for every item\n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            # Get the next review and its correct label\n",
    "            review = training_reviews[i]\n",
    "            label = labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "            \n",
    "            # Hidden layer\n",
    "            # no activation function to preserve the linearity\n",
    "            ## Add in only the weights for non-zero items\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]            \n",
    "\n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(np.matmul(self.layer_1, self.weights_1_2))       \n",
    "              \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "            \n",
    "            # Output error\n",
    "            layer_2_error =  self.get_target_for_label(label) - layer_2   \n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2) # adjust for the slope of non-linearity\n",
    "            \n",
    "            # Backpropagated error\n",
    "            layer_1_error = np.matmul(layer_2_delta, self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            # note in feedforward weights_1_2 is multiplied with layer_1 as input while in backpopogation its transpose \n",
    "            # is multiplied with downstream (next layer) error term as the input.\n",
    "            layer_1_delta = layer_1_error * 1 # hidden layer gradients - no nonlinearity so it's the same as the error (no adjustment)\n",
    "            \n",
    "            # update weights   \n",
    "            self.weights_1_2 += self.learning_rate * layer_2_delta * self.layer_1.T # update hidden-to-output weights with gradient descent step\n",
    "\n",
    "            ## Only update the weights that were used in the forward pass\n",
    "            for index in review:\n",
    "                self.weights_0_1[index] += self.learning_rate * layer_1_delta[0] # update input-to-hidden weights with gradient descent step\n",
    "            \n",
    "            # Keep track of corrcet label predictions\n",
    "            # how accurate are the predictions\n",
    "            if abs(layer_2_error) < 0.5:\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            # how fast we are training    \n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0): # every 2500, make a new line\n",
    "                print(\"\")\n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \"\"\"\n",
    "        Attempts to predict the labels for the given testing_reviews,\n",
    "        and uses the test_labels to calculate the accuracy of those predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep track of how many correct predictions we make\n",
    "        correct = 0\n",
    "\n",
    "        # we'll time how many predictions per second we make\n",
    "        start = time.time()\n",
    "\n",
    "        # Loop through each of the given reviews and call run to predict its label. \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            # For debug purposes, print out our prediction accuracy and speed \n",
    "            # throughout the prediction process. \n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \"\"\"\n",
    "        Returns a POSITIVE or NEGATIVE prediction for the given review.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a forward pass through the network, like in the \"train\" function.\n",
    "\n",
    "        # Hidden Layer\n",
    "        ## Identify the indices used in the review and then add just those weights to layer_1 \n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output Layer\n",
    "        layer_2 = self.sigmoid(np.matmul(self.layer_1, self.weights_1_2))\n",
    "        \n",
    "        # Return POSITIVE for values above greater-than-or-equal-to 0.5 in the output layer;\n",
    "        # return NEGATIVE for other values\n",
    "        \n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(reviews)*split_frac)\n",
    "train_x, remaining_x = reviews[:split_idx], reviews[split_idx:]\n",
    "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:12.5% Speed(reviews/sec):1439. #Correct:1943 #Trained:2501 Training Accuracy:77.6%\n",
      "Progress:25.0% Speed(reviews/sec):1386. #Correct:4002 #Trained:5001 Training Accuracy:80.0%\n",
      "Progress:37.5% Speed(reviews/sec):1390. #Correct:6118 #Trained:7501 Training Accuracy:81.5%\n",
      "Progress:50.0% Speed(reviews/sec):1398. #Correct:8272 #Trained:10001 Training Accuracy:82.7%\n",
      "Progress:62.5% Speed(reviews/sec):1399. #Correct:10432 #Trained:12501 Training Accuracy:83.4%\n",
      "Progress:75.0% Speed(reviews/sec):1398. #Correct:12553 #Trained:15001 Training Accuracy:83.6%\n",
      "Progress:87.5% Speed(reviews/sec):1397. #Correct:14677 #Trained:17501 Training Accuracy:83.8%\n",
      "Progress:99.9% Speed(reviews/sec):1390. #Correct:16867 #Trained:20000 Training Accuracy:84.3%\n",
      "Total training time is 0.28 minutes.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "mlp_full = SentimentNetwork(train_x,train_y,min_count=0,polarity_cutoff=0.0,learning_rate=0.01)\n",
    "\n",
    "# start the clock\n",
    "start_time = time.time()\n",
    "# call th etrain function\n",
    "\n",
    "mlp_full.train(train_x, train_y)\n",
    "# print the training time\n",
    "print(\"\\nTotal training time is %s minutes.\" % round((time.time() - start_time) / 60, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):2287. #Correct:2194 #Tested:2500 Testing Accuracy:87.7%"
     ]
    }
   ],
   "source": [
    "mlp_full.test(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):2309. #Correct:2158 #Tested:2500 Testing Accuracy:86.3%"
     ]
    }
   ],
   "source": [
    "mlp_full.test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative solution: Sentiment Analysis with an RNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "\n",
    "## Encoding the words\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True) # the most common word mapped to the integer value 1 \n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # start from 1 expilicitly stated\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "\n",
    "## Encoding the labels\n",
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n",
    "\n",
    "## Removing Outliers\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length of not 0 that we want to keep\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "## Padding sequences\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features) * split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "valid_frac = 0.5\n",
    "test_idx = int(len(remaining_x) * valid_frac)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='assets/network_diagram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # our word tokens +1 for the 0-token added to pad input features +\n",
    "output_size = 1 # a single sigmoid value between 0 and 1  \n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.574357... Val Loss: 0.592211\n",
      "Epoch: 1/4... Step: 200... Loss: 0.594059... Val Loss: 0.663425\n",
      "Epoch: 1/4... Step: 300... Loss: 0.523350... Val Loss: 0.585923\n",
      "Epoch: 1/4... Step: 400... Loss: 0.554656... Val Loss: 0.571660\n",
      "Epoch: 2/4... Step: 500... Loss: 0.521586... Val Loss: 0.575272\n",
      "Epoch: 2/4... Step: 600... Loss: 0.357778... Val Loss: 0.509176\n",
      "Epoch: 2/4... Step: 700... Loss: 0.385232... Val Loss: 0.525697\n",
      "Epoch: 2/4... Step: 800... Loss: 0.406087... Val Loss: 0.461405\n",
      "Epoch: 3/4... Step: 900... Loss: 0.305346... Val Loss: 0.490994\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.224487... Val Loss: 0.455219\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.340713... Val Loss: 0.462203\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.231399... Val Loss: 0.469280\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.108195... Val Loss: 0.480258\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.201755... Val Loss: 0.509962\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.165618... Val Loss: 0.505271\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.071419... Val Loss: 0.564402\n",
      "\n",
      "Total training time is 76.03 minutes.\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "start = time.time()\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.detach() for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.detach() for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            \n",
    "print(\"\\nTotal training time is %s minutes.\" % round((time.time() - start_time) / 60, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.578\n",
      "Test accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.detach() for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
